{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df733797",
   "metadata": {},
   "source": [
    "## Creating the dataset.json file for the nnuNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4387825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === Set root path containing both 'imagesTr' and 'labelsTr' ===\n",
    "root_dir = r\"------ INSERT PATH HERE ----------\"\n",
    "images_dir = os.path.join(root_dir, \"imagesTr\")\n",
    "labels_dir = os.path.join(root_dir, \"labelsTr\")\n",
    "\n",
    "# === Gather image-label pairs ===\n",
    "image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(\"_0000.nii.gz\")])\n",
    "label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith(\".nii.gz\") and not f.endswith(\"_0000.nii.gz\")])\n",
    "\n",
    "# === Sanity check ===\n",
    "if len(image_files) != len(label_files):\n",
    "    print(f\"⚠️ Warning: Number of images ({len(image_files)}) and labels ({len(label_files)}) does not match.\")\n",
    "\n",
    "# === Construct training list ===\n",
    "training = []\n",
    "for img, lbl in zip(image_files, label_files):\n",
    "    training.append({\n",
    "        \"image\": f\"./imagesTr/{img}\",\n",
    "        \"label\": f\"./labelsTr/{lbl}\"\n",
    "    })\n",
    "\n",
    "# === Build dataset dictionary ===\n",
    "dataset_dict = {\n",
    "    \"name\": \"LungLobeCombinedDataset134\",\n",
    "    \"description\": \"Segmentation of lung lobes from preoperative CT scans (combined public dataset).\",\n",
    "    \"tensorImageSize\": \"3D\",\n",
    "    \"modality\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"background\": 0,\n",
    "        \"RUL\": 1,\n",
    "        \"RML\": 2,\n",
    "        \"RLL\": 3,\n",
    "        \"LUL\": 4,\n",
    "        \"LLL\": 5\n",
    "    },\n",
    "    \"numTraining\": len(training),\n",
    "    \"numTest\": 0,\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"file_ending\": \".nii.gz\",\n",
    "    \"training\": training,\n",
    "    \"test\": []\n",
    "}\n",
    "\n",
    "# === Save to dataset.json ===\n",
    "output_path = os.path.join(root_dir, \"dataset.json\")\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(dataset_dict, f, indent=4)\n",
    "\n",
    "print(f\"✅ dataset.json created with {len(training)} training samples at:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398bcb2",
   "metadata": {},
   "source": [
    "## dataset.json for the Fissure datasets for the nnuNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9554b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Set root path containing both 'imagesTr' and 'labelsTr' ===\n",
    "root_dir = r\"------ INSERT PATH HERE ------\"\n",
    "images_dir = os.path.join(root_dir, \"imagesTr\")\n",
    "labels_dir = os.path.join(root_dir, \"labelsTr\")\n",
    "\n",
    "# Fissure configs: suffix in label filenames and dataset naming\n",
    "FISSURES = [\n",
    "    {\"suffix\": \"RHF\", \"name\": \"RightHorizontalFissure\"},\n",
    "    {\"suffix\": \"ROF\", \"name\": \"RightObliqueFissure\"},\n",
    "    {\"suffix\": \"LOF\", \"name\": \"LeftObliqueFissure\"},\n",
    "]\n",
    "\n",
    "def build_dataset_for_fissure(fissure_suffix: str, dataset_name: str):\n",
    "    # Gather images: <ID>_0000.nii.gz\n",
    "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(\"_0000.nii.gz\")])\n",
    "    # Gather labels: <ID>_{suffix}.nii.gz\n",
    "    label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith(f\"_{fissure_suffix}.nii.gz\")])\n",
    "\n",
    "    # Build index by ID for robust matching\n",
    "    def img_id(fname):  # e.g., LLS0020_0000.nii.gz -> LLS0020\n",
    "        return fname.replace(\"_0000.nii.gz\", \"\")\n",
    "    def lbl_id(fname):  # e.g., LLS0020_RHF.nii.gz  -> LLS0020\n",
    "        return fname.replace(f\"_{fissure_suffix}.nii.gz\", \"\")\n",
    "\n",
    "    images_by_id = {img_id(f): f for f in image_files}\n",
    "    labels_by_id = {lbl_id(f): f for f in label_files}\n",
    "\n",
    "    # Intersect IDs\n",
    "    common_ids = sorted(set(images_by_id.keys()) & set(labels_by_id.keys()))\n",
    "    missing_imgs = sorted(set(labels_by_id.keys()) - set(images_by_id.keys()))\n",
    "    missing_lbls = sorted(set(images_by_id.keys()) - set(labels_by_id.keys()))\n",
    "\n",
    "    if missing_imgs:\n",
    "        print(f\"⚠️ [{fissure_suffix}] Labels without images: {missing_imgs}\")\n",
    "    if missing_lbls:\n",
    "        print(f\"⚠️ [{fissure_suffix}] Images without labels: {missing_lbls}\")\n",
    "\n",
    "    # Construct training list\n",
    "    training = []\n",
    "    for cid in common_ids:\n",
    "        training.append({\n",
    "            \"image\": f\"./imagesTr/{images_by_id[cid]}\",\n",
    "            \"label\": f\"./labelsTr/{labels_by_id[cid]}\"\n",
    "        })\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"name\": f\"LungFissure_{dataset_name}\",\n",
    "        \"description\": f\"Binary segmentation of {dataset_name} (0=background, 1=fissure) on coronal-derived masks.\",\n",
    "        \"tensorImageSize\": \"3D\",\n",
    "        \"modality\": { \"0\": \"CT\" },\n",
    "        \"labels\": { \"background\": 0, \"fissure\": 1 },\n",
    "        \"numTraining\": len(training),\n",
    "        \"numTest\": 0,\n",
    "        \"channel_names\": { \"0\": \"CT\" },\n",
    "        \"file_ending\": \".nii.gz\",\n",
    "        \"training\": training,\n",
    "        \"test\": []\n",
    "    }\n",
    "\n",
    "    out_path = os.path.join(root_dir, f\"dataset_{fissure_suffix}.json\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(dataset_dict, f, indent=4)\n",
    "    print(f\"✅ [{fissure_suffix}] dataset.json created with {len(training)} pairs at:\\n{out_path}\")\n",
    "\n",
    "# Build datasets for all three fissures\n",
    "for cfg in FISSURES:\n",
    "    build_dataset_for_fissure(cfg[\"suffix\"], cfg[\"name\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
